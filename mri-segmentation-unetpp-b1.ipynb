{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11030732,"sourceType":"datasetVersion","datasetId":6869931}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e94d086f-a227-44dc-afd2-0d2a2dceb4be","cell_type":"markdown","source":"# Загрузка библиотек\n\nЕсли какие-то из библиотек не установлены - используйте !pip install\n\nДля работы на Kaggle требуется верифицированный по номеру телефона профиль (без него не сработает pip install (точнее, нельзя будет в настройках включить работу с интернетом) и не подключить графические ускорители). Если при попытке верификации по номеру выдает ошибку \"this phone number can't be verified\" - попробуйте обратиться в техподдержку Kaggle, они ответят в течении 1-2 дней и уберут ошибку при вводе того же номера (личный опыт).\n\nНа удаленных средах (Kaggle, Google Colab), для работы с данными/модификации преобразований изображений, графические ускорители можно не подключать, и работать в стандартной среде. В противном случае - перед запуском кода подлючите графические ускорители (в Kaggle: Session Options -> Accelerator -> GPU T4 x 2, в Google Colab - меню рядом со статусом ОЗУ/Диск -> Сменить среду выполнения -> Графический ускоритель T4).","metadata":{"id":"e94d086f-a227-44dc-afd2-0d2a2dceb4be"}},{"id":"b4072585-d2ce-4bda-8feb-e5b4f6da767b","cell_type":"code","source":"!pip install colorama","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b5fb5167-c366-43b9-b61c-7ffa2f058545","cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7584a208-a093-46fb-b52c-c90b78fda5e6","cell_type":"code","source":"!pip install GPUtil","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f53b6924-5799-4f7b-9d32-ab5400a7569e","cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport random\nimport glob\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\n\n# visualization\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom joblib import Parallel, delayed\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_  = Fore.GREEN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nimport sys\nimport h5py\nfrom sklearn.preprocessing import StandardScaler\n\n#Модели машинного обучения\nimport segmentation_models_pytorch as smp\n\n#Утилиты для работы с GPU\nimport GPUtil","metadata":{"id":"f53b6924-5799-4f7b-9d32-ab5400a7569e","outputId":"f5833cff-b8f0-46ff-84d3-c74bd69bffac","colab":{"base_uri":"https://localhost:8080/","height":383},"trusted":true},"outputs":[],"execution_count":null},{"id":"6a523477-188f-4f18-b219-521ffda83df0","cell_type":"markdown","source":"# Загрузка данных","metadata":{"id":"6a523477-188f-4f18-b219-521ffda83df0"}},{"id":"eb758ce0-0aa3-4de2-8255-03cc7583aa2d","cell_type":"markdown","source":"Перед работой с данными, их нужно загрузить на компьютер/удаленную среду.\n\nДля загрузки на компьютер, используйте скрипт для загрузки (находится в другом jupyter notebook).\n\nДля загрузки на Google Colab с компьютера ни в коем случае не используйте внутрисессионное хранилище - данные пропадут при смене среды (а грузится они будут очень долго), загрузите данные на Google Drive в архиве .zip (во первых так данные будут меньше, а во вторых процедура \"загрузка .zip с Google Drive в Google Colab -> распаковка .zip в Google Colab\" быстрее процедуры \"загрузи все несжатые данные с Google Drive\", в третьих в стандартный бесплатный объем Google Drive несжатые данные не влезут).\n\nДля загрузки на Kaggle с компьютера испольйте в меню справа Input -> Upload. Советую перед загрузкой положить данные в .zip архив, это уменьшит время загрузки данных. Kaggle самостоятельно распакует архив и создаст датасет для работы.\n\nВ данной реализации сегментации, используются только Cor снимки, поэтому, при желании, можно не загружать другие типы снимков (это существенно уменьшит вес загружаеммых данных).","metadata":{"id":"eb758ce0-0aa3-4de2-8255-03cc7583aa2d"}},{"id":"da49ee47-fddd-49e2-9c9e-1fda9c67eafc","cell_type":"code","source":"#Преобразует tif файлы в массив масок и снимков\ndef load_tif(x_path, y_path,count_dict, step=4):\n    images = []\n    masks = []\n    image = Image.open(x_path)\n    mask = Image.open(y_path)\n    i = 0\n    cnt = 0\n\n    while True:\n        try:\n            mask.seek(i)\n            mask_array = np.array(mask)\n            image.seek(i)\n            image_array = np.array(image)\n            if sum(sum(mask_array)) > 0: #Проверка, что маска не полностью черная\n\n                #Если маска не из 0 и 1, а из, например, 0 и 255, меняем 255 на 1.\n                mask_array[mask_array > 1] = 1\n\n                masks.append(mask_array)\n                images.append(image_array)\n            cnt+=1\n\n            i += 1\n        except EOFError:\n            break\n\n    if 'T1' in x_path:\n        count_dict['scan'].append('T1')\n    elif 'T2' in x_path:\n        count_dict['scan'].append('T2')\n    count_dict['count'].append(cnt)\n\n    return images, masks","metadata":{"id":"da49ee47-fddd-49e2-9c9e-1fda9c67eafc","trusted":true},"outputs":[],"execution_count":null},{"id":"85db74c4-9916-4f63-ba62-28472addbeca","cell_type":"code","source":"#Достаем изображения и маски из многослойного tif файла\ndef get_images(x_pathes, y_pathes):\n\n    images =[]\n    masks = []\n    count_dict = {'scan':[],'count':[],}\n    for x_path, y_path in zip(x_pathes, y_pathes):\n\n        images_sample,mask_sample = load_tif(x_path, y_path,count_dict)\n        for im,ms in zip(images_sample,mask_sample):\n\n            images.append(im)\n            masks.append(ms)\n\n    print(len(count_dict['count']))\n\n    return images, masks ,count_dict","metadata":{"id":"85db74c4-9916-4f63-ba62-28472addbeca","trusted":true},"outputs":[],"execution_count":null},{"id":"2629882e-8f0d-41ea-bdb4-bfe558d74bb7","cell_type":"code","source":"#Обходит директорию и сохранятет пути пациентов, вычленяет проекции Cor T1 и Cor T2 и маски\ndef get_pathes(path):\n    x_pathes_all = []\n    y_pathes_all = []\n    for patient in os.listdir(path):\n        x_pathes = []\n        y_pathes = []\n\n        for ID_s in os.listdir(path + '/'+ patient ):\n            if 'ID' in ID_s:\n                msk_t1 = 0\n                msk_t2 = 0\n                for tif_name in os.listdir(path + '/'+ patient + '/'+ID_s):\n\n                    if 'Cor' in tif_name:\n                        if 'T1' in tif_name:\n\n                            if  'mask' not in tif_name.lower():\n                                x_pathes.append(path + '/'+ patient + '/' + ID_s + '/'+ tif_name)\n\n                            elif 'mask' in tif_name.lower():\n                                msk_t1 = 1\n                                y_pathes.append(path + '/'+ patient + '/'+ID_s + '/' + tif_name)\n\n                        elif 'T2' in tif_name:\n\n                            if 'mask' not in tif_name.lower():\n                                x_pathes.append(path + '/'+ patient + '/'+ID_s + '/' + tif_name)\n\n                            elif 'mask' in tif_name.lower():\n                                msk_t2 = 1\n                                y_pathes.append(path + '/'+ patient + '/'+ID_s + '/' + tif_name)\n\n                if msk_t1==0:\n                    x_pathes.pop()\n                if msk_t2==0:\n                    x_pathes.pop()\n\n        x_pathes_all.append(x_pathes)\n        y_pathes_all.append(y_pathes)\n\n    return x_pathes_all, y_pathes_all","metadata":{"id":"2629882e-8f0d-41ea-bdb4-bfe558d74bb7","trusted":true},"outputs":[],"execution_count":null},{"id":"5d6fec08-db2e-4dbf-bb84-ba63364f5a23","cell_type":"code","source":"def flatten(xss): # для развертки и однородности списков\n\n    return [x for xs in xss for x in xs]","metadata":{"id":"5d6fec08-db2e-4dbf-bb84-ba63364f5a23","trusted":true},"outputs":[],"execution_count":null},{"id":"e631da6f-5afe-40c8-b4a2-436866b72803","cell_type":"markdown","source":"Указываем путь, где хранится папка с данными. Для корректной работы, данные должны быть в таком же формате, как формирует скрипт для скачки данных. Данный формат представляет собой папку, где данные каждого пациента хранятся в папках \"ID_{номер_пациента}\", в каждой такой папке хранится папка \"ID {номер пациента}\" с .tif изображениями, а также файл labels.txt с категориями Knosp. В текущей реализации сегментации, файлы labels.txt, а также все не коронарные снимки (без Cor в названии) не используются, поэтому, при работе с удаленной средой (Kaggle/Google colab) их можно не загружать (это в несколько раз уменьшит вес загружаемого файла с данными)","metadata":{"id":"e631da6f-5afe-40c8-b4a2-436866b72803"}},{"id":"8e1618f3-c1c8-42bc-9f6d-ebdd57dabdef","cell_type":"code","source":"#Указываем путь до папки с данными. Формат данных - как формирует скрипт загрузки датасета\n\n#Примеры пути для разных сред\n\n#При хранении на Google Drive при работе с Google Colab (в данном примере - данные не сжаты в .zip архиве)\n#from google.colab import drive\n#drive.mount('/content/drive')\n#x_pth, y_pth = get_pathes('/content/drive/MyDrive/data_pituitary_test_light')\n\n#Формат ссылки при хранении на Kaggle\nx_pth ,y_pth = get_pathes('/kaggle/input/data-22-02-25-cls108-seg72')\n\n#Формат ссылки, при хранении на локальном компьютере\n#x_pth, y_pth = get_pathes('C:/Users/12345654321/data_pituitary_18_01_25')","metadata":{"id":"8e1618f3-c1c8-42bc-9f6d-ebdd57dabdef","trusted":true},"outputs":[],"execution_count":null},{"id":"9f0cd344-2d4a-45cc-ac4f-25ef3131fd4f","cell_type":"code","source":"x_pth ,y_pth = [flatten(i) for i in [x_pth ,y_pth]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ec7a7819-3561-46d2-970e-ed23b9f00957","cell_type":"code","source":"im,ms,cnt= get_images(x_pth ,y_pth)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"31563172-3718-4507-ae5f-ec18c4779ebc","cell_type":"code","source":"#Разбиваем данные на train и validation, при желании можно поменять test_size.\n\nx_pth_train,x_pth_val,y_pth_train,y_pth_val = train_test_split(x_pth , y_pth, test_size=0.14)","metadata":{"id":"31563172-3718-4507-ae5f-ec18c4779ebc","trusted":true},"outputs":[],"execution_count":null},{"id":"d22dcf3f-8b9c-4df3-81c8-0264f48713de","cell_type":"code","source":"#Достает изображения и маски, преобразуя в массивы.\n\n#Первая цифра output - кол-во tif файлов в test датасете (по 2 файла (Cor T1 и Cor T2) на пациента, не включая маски)\n#Вторая цифра output - кол-во tif файлов в train датасете (по 2 файла (Cor T1 и Cor T2) на пациента, не включая маски)\n\nx_val,y_val,_ = get_images(x_pth_val ,y_pth_val)\n\nx_train,y_train,_= get_images(x_pth_train ,y_pth_train)","metadata":{"id":"d22dcf3f-8b9c-4df3-81c8-0264f48713de","trusted":true},"outputs":[],"execution_count":null},{"id":"7e6d2e97-6b29-4780-9efa-5d6ad5284cb6","cell_type":"markdown","source":"# Преобразование данных","metadata":{"id":"7e6d2e97-6b29-4780-9efa-5d6ad5284cb6"}},{"id":"94b2e555-a67b-43f2-bf0b-65265a84de11","cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset): # загружает в оперативную память\n\n    def __init__(self, X,y, label=True, transforms=None):\n        self.label      = label\n        self.img_paths  = X\n        self.msk_paths  = y\n        self.transforms = transforms\n\n    def __len__(self):\n\n        return len(self.img_paths)\n\n    def __getitem__(self, index):\n\n        if self.label == True:\n\n            img = self.img_paths[index]\n            msk = self.msk_paths[index]\n\n            if self.transforms:\n\n                data = self.transforms(image=np.array(img/255., dtype=np.float32)  , mask=np.array(msk, dtype=np.float32))\n                img  = data['image']\n                msk  = data['mask']\n\n            return torch.tensor([img]), torch.tensor([msk])\n\n        else:\n\n            img = self.img_paths[index]\n            if self.transforms:\n                data = self.transforms(image=np.array(img/255., dtype=np.float32))\n                img  = data['image']\n            return torch.tensor([img])","metadata":{"id":"94b2e555-a67b-43f2-bf0b-65265a84de11","trusted":true},"outputs":[],"execution_count":null},{"id":"c5a103d7-f2e6-46d9-9029-a0633838bd48","cell_type":"markdown","source":"Здесь происходит преобразование данных с изображений (их обрезка, нормализация итд). Можно попробовать поизменять тут параметры для улучшения качества обучения. Будьте внимательны - не все преобразования можно не дублировать в valid. Повороты/dropout части картинки для улучшения качества обучения - норм, существенная обрезка изображения/изменение яркостей пикселей - не норм, дублируйте в valid.","metadata":{"id":"c5a103d7-f2e6-46d9-9029-a0633838bd48"}},{"id":"85c40926-db1b-459d-ae2a-310d12c4bac5","cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.augmentations.crops.transforms.CenterCrop(256,256),\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        A.CoarseDropout(max_holes=8, max_height=224//20, max_width=224//20,\n                         min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n\n\n        A.HorizontalFlip(p=0.5),\n#         A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n        A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n            # A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n        ], p=0.25),\n        # A.ColorJitter(brightness=0, contrast=0.0002, saturation=0, hue=0.2, always_apply=False, p=0.5),\n        # A.CoarseDropout(max_holes=8, max_height=224//20, max_width=224//20,\n        #                  min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n        A.augmentations.Normalize(mean=(0.485, ), std=(0.229, )),\n        ], p=1.0),\n\n    \"valid\": A.Compose([\n        A.augmentations.crops.transforms.CenterCrop(256,256),\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        A.augmentations.Normalize(mean=(0.485, ), std=(0.229, )),\n        ], p=1.0)\n}","metadata":{"id":"85c40926-db1b-459d-ae2a-310d12c4bac5","trusted":true},"outputs":[],"execution_count":null},{"id":"220536e4-7e36-4e05-a5ad-61f01176c6e8","cell_type":"markdown","source":"Подготовка данных для загрузки в модель. Здесь можно поменять batch_size, это может повлиять на обучение.","metadata":{"id":"220536e4-7e36-4e05-a5ad-61f01176c6e8"}},{"id":"c64985e2-c7c7-480c-973c-eb62f69afd02","cell_type":"code","source":"train_dataset = BuildDataset(x_train,y_train, transforms=data_transforms['train'])\nvalid_dataset = BuildDataset(x_val,y_val, transforms=data_transforms['valid'])\n\ntrain_batch_size = 16\nvalid_batch_size = 32\n\ntrain_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False)","metadata":{"id":"c64985e2-c7c7-480c-973c-eb62f69afd02","trusted":true},"outputs":[],"execution_count":null},{"id":"f529436f-1b68-4a4e-b29e-ebeb33622dd6","cell_type":"markdown","source":"Здесь происходит визуализация преобразованных данных. Используйте чтобы примерно оценить, что произошло с данными после преобразования/найти ошибки в коде.","metadata":{"id":"f529436f-1b68-4a4e-b29e-ebeb33622dd6"}},{"id":"b758e7cc-d875-4c4b-b65d-50f6826b842d","cell_type":"code","source":"def plot_batch(imgs, msks, size=3):\n    plt.figure(figsize=(5*5, 5))\n    for idx in range(size):\n        plt.subplot(1, size, idx+1)\n        img = imgs[idx,].permute((1, 2, 0)).numpy()*255.0\n        msk = msks[idx,].permute((1, 2, 0)).numpy()\n        show_img(img, msk)\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"b758e7cc-d875-4c4b-b65d-50f6826b842d","trusted":true},"outputs":[],"execution_count":null},{"id":"d2a559b6-5680-47a0-911f-7fd43a996512","cell_type":"code","source":"def show_img(img, mask=None):\n\n    plt.imshow(img, cmap='bone')\n\n    plt.imshow(mask, alpha=0.5)\n    handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0)]]\n    labels = [\"Adenoma\"]\n    plt.legend(handles,labels)\n    plt.axis('off')","metadata":{"id":"d2a559b6-5680-47a0-911f-7fd43a996512","trusted":true},"outputs":[],"execution_count":null},{"id":"e59ae079-9f89-436c-9e16-9f9a5cf34e50","cell_type":"code","source":"#Изображение после train преобразования + маска.\n\nimgs, msks = next(iter(train_loader))\nplot_batch(imgs, msks, size=5)","metadata":{"id":"e59ae079-9f89-436c-9e16-9f9a5cf34e50","trusted":true},"outputs":[],"execution_count":null},{"id":"fc9a3f9a-2996-44bf-839b-8a4e0a12fd07","cell_type":"code","source":"#Изображение после train преобразования\n\nplot_batch(imgs, imgs, size=5)","metadata":{"id":"fc9a3f9a-2996-44bf-839b-8a4e0a12fd07","trusted":true},"outputs":[],"execution_count":null},{"id":"e465a7b6-a7c3-40fa-907f-f3e9a03b84ba","cell_type":"code","source":"#Маска\n\nplot_batch(msks, msks, size=5)","metadata":{"id":"e465a7b6-a7c3-40fa-907f-f3e9a03b84ba","trusted":true},"outputs":[],"execution_count":null},{"id":"17c507fb-02c4-47df-a631-59df25aa3741","cell_type":"markdown","source":"# Модель","metadata":{"id":"17c507fb-02c4-47df-a631-59df25aa3741"}},{"id":"446e5c44-f371-4ccd-aac3-c0217e9f9325","cell_type":"code","source":"#Принудительно включаем garbage collector (чтобы улучшить производительность обучения)\n\ngc.collect()","metadata":{"id":"446e5c44-f371-4ccd-aac3-c0217e9f9325","trusted":true},"outputs":[],"execution_count":null},{"id":"946d9a7d-a29e-4104-85b6-08785170d881","cell_type":"markdown","source":"Архитектура модели. Используются реализации из библиотеки segmentation_models_pytorch. Саму модель/ее параметры (encoder_name) можно поменять.","metadata":{"id":"946d9a7d-a29e-4104-85b6-08785170d881"}},{"id":"f11c96c5-0219-42cd-9a4c-1f0701e1ea73","cell_type":"code","source":"def build_model():\n    model = smp.UnetPlusPlus(\n        encoder_name='efficientnet-b1' ,\n                  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n        encoder_weights='imagenet',# use `imagenet` pre-trained weights for encoder initialization\n        in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n        classes=1   ,     # model output channels (number of classes in your dataset)\n        activation=None,\n    )\n    model.to('cuda')\n    return model\n\ndef load_model(path):\n    model = build_model()\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model","metadata":{"id":"f11c96c5-0219-42cd-9a4c-1f0701e1ea73","trusted":true},"outputs":[],"execution_count":null},{"id":"2db0b2cb-4df6-4c3c-9a57-40237f673d79","cell_type":"markdown","source":"# Loss","metadata":{"id":"2db0b2cb-4df6-4c3c-9a57-40237f673d79"}},{"id":"5a2f8f10-130f-4fea-9c71-74571d5f64fb","cell_type":"markdown","source":"Здесь настраивается Loss (функция criterion), также есть фунции ручного расчета dice_coef и iou_coef.","metadata":{"id":"5a2f8f10-130f-4fea-9c71-74571d5f64fb"}},{"id":"1ae589a7-d104-4c7a-b0d1-4a04d1d45354","cell_type":"code","source":"JaccardLoss = smp.losses.JaccardLoss(mode='binary')\nDiceLoss    = smp.losses.DiceLoss(mode='binary')\nBCELoss     = smp.losses.SoftBCEWithLogitsLoss()\nLovaszLoss  = smp.losses.LovaszLoss(mode='binary', per_image=False)\nTverskyLoss = smp.losses.TverskyLoss(mode='binary', log_loss=False)\n\ndef dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n    return dice\n\ndef iou_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n    return iou\n\ndef criterion(y_pred, y_true):\n    return BCELoss(y_pred, y_true) + TverskyLoss(y_pred, y_true)","metadata":{"id":"1ae589a7-d104-4c7a-b0d1-4a04d1d45354","trusted":true},"outputs":[],"execution_count":null},{"id":"5c8fc0de-d7dc-43c2-b5a0-019f48bc0bda","cell_type":"markdown","source":"# Обучение","metadata":{"id":"5c8fc0de-d7dc-43c2-b5a0-019f48bc0bda"}},{"id":"79240d03-98ca-4cb0-9a56-6aca78a3be31","cell_type":"code","source":"#Обучение одной эпохи\n\ndef train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    train_scores = []\n    dataset_size = 0\n    running_loss = 0.0\n\n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:\n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n\n        batch_size = images.size(0)\n\n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n\n            loss   = criterion(y_pred, masks)\n            if (loss < 0):\n                print(\"ALARM\")\n                print(loss)\n                print(\"#######\")\n\n            loss   = loss / CFG.n_accumulate\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n\n        y_pred = (nn.Sigmoid()(y_pred)).double()\n        train_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n        vtrain_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n        train_scores.append([train_dice, vtrain_jaccard])\n\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n\n    train_scores  = np.mean(train_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return epoch_loss,train_scores","metadata":{"id":"79240d03-98ca-4cb0-9a56-6aca78a3be31","trusted":true},"outputs":[],"execution_count":null},{"id":"69cfabee-a9bb-4678-b6ff-c6319fe4c4fc","cell_type":"code","source":"#Валидация одной эпохи\n\n@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n\n    dataset_size = 0\n    running_loss = 0.0\n\n    val_scores = []\n\n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (images, masks) in pbar:\n        images  = images.to(device, dtype=torch.float)\n        masks   = masks.to(device, dtype=torch.float)\n\n        batch_size = images.size(0)\n\n        y_pred  = model(images)\n        loss    = criterion(y_pred, masks)\n\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n\n        epoch_loss = running_loss / dataset_size\n\n        y_pred = (nn.Sigmoid()(y_pred)).double()\n        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n        val_scores.append([val_dice, val_jaccard])\n\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_memory=f'{mem:0.2f} GB')\n\n\n    val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return epoch_loss, val_scores","metadata":{"id":"69cfabee-a9bb-4678-b6ff-c6319fe4c4fc","trusted":true},"outputs":[],"execution_count":null},{"id":"0b27ae09-65fa-4f56-90bd-1454c2e6d0ae","cell_type":"code","source":"#Процесс обучения\n\ndef run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    # wandb.watch(model, log_freq=100)\n\n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n\n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_dice      = -np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n\n    for epoch in range(1, num_epochs + 1):\n        gc.collect()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss,train_scores = train_one_epoch(model, optimizer, scheduler,\n                                           dataloader=train_loader,\n                                           device=CFG.device, epoch=epoch)\n        train_dice,train_jaccard = train_scores\n\n        print(f'Train Dice: {train_dice:0.4f} | Train Jaccard: {train_jaccard:0.4f}')\n\n        val_loss, val_scores = valid_one_epoch(model, valid_loader,\n                                                 device=CFG.device,\n                                                 epoch=epoch)\n        val_dice, val_jaccard = val_scores\n\n        history['Train Loss'].append(train_loss)\n        history['Train Dice'].append(train_dice)\n        history['Train Jaccard'].append(train_jaccard)\n        history['Valid Loss'].append(val_loss)\n        history['Valid Dice'].append(val_dice)\n        history['Valid Jaccard'].append(val_jaccard)\n\n        # Log the metrics\n        # wandb.log({\"Train Loss\": train_loss,\n        #            \"Valid Loss\": val_loss,\n        #            \"Valid Dice\": val_dice,\n        #            \"Valid Jaccard\": val_jaccard,\n        #            \"LR\":scheduler.get_last_lr()[0]})\n\n        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n\n        # deep copy the model\n        if val_dice > best_dice:\n            print(f\"{c_}Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n            best_dice    = val_dice\n            best_jaccard = val_jaccard\n            best_epoch   = epoch\n            run.summary[\"Best Dice\"]    = best_dice\n            run.summary[\"Best Jaccard\"] = best_jaccard\n            run.summary[\"Best Epoch\"]   = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"best_epoch-{fold:02d}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            # wandb.save(PATH)\n            print(f\"Model Saved{sr_}\")\n\n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = f\"last_epoch-{fold:02d}.bin\"\n        torch.save(model.state_dict(), PATH)\n\n        print(); print()\n\n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Score: {:.4f}\".format(best_jaccard))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n\n    return model, history","metadata":{"id":"0b27ae09-65fa-4f56-90bd-1454c2e6d0ae","trusted":true},"outputs":[],"execution_count":null},{"id":"04a031b4-9c53-477a-8418-813343946526","cell_type":"markdown","source":"Параметры модели.\n\nДля изменения числа эпох, меняйте поле epochs. Для тестовых прогонов хватает 50-70 эпох. Для финального, в предыдущих работах используется 200 эпох.\n\nДля изменения параметров планировщика, редактируйте поля scheduler, T_0, T_max, min_lr.\n\nНачальный lr (или lr без планировщика) - поле lr.","metadata":{"id":"04a031b4-9c53-477a-8418-813343946526"}},{"id":"1a48b536-7504-41ab-a32e-c9e5f9d6eb79","cell_type":"code","source":"class CFG:\n    seed          = 42\n    debug         = False # set debug=False for Full Training\n    exp_name      = 'Baselinev2'\n    comment       = 'unetpp-efficientnet_b1-224x224-aug2-split2'\n    model_name    = 'UnetPlusPlus'\n    backbone      = 'efficientnet-b1'\n    train_bs      = 128\n    valid_bs      = train_bs*2\n    img_size      = [224, 224]\n    epochs        = 70\n    lr            = 1e-4\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 1e-6\n    T_max         = int(30000/train_bs*epochs)+50\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 1e-6\n    n_accumulate  = max(1, 32//train_bs)\n    n_fold        = 5\n    num_classes   = 3\n    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a4673215-6853-4a25-9f79-9ff9aa9fc36b","cell_type":"markdown","source":"Настройки параметров планировщика изменения lr со временем","metadata":{"id":"a4673215-6853-4a25-9f79-9ff9aa9fc36b"}},{"id":"c96585b9-6bdd-4d14-8574-781d088dd5d2","cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CFG.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max,\n                                                   eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0,\n                                                             eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=CFG.min_lr,)\n    elif CFG.scheduer == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif CFG.scheduler == None:\n        return None\n\n    return scheduler","metadata":{"id":"c96585b9-6bdd-4d14-8574-781d088dd5d2","trusted":true},"outputs":[],"execution_count":null},{"id":"0790ba5e-dca9-4ded-936c-849466df2c95","cell_type":"code","source":"model = build_model()\noptimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\nscheduler = fetch_scheduler(optimizer)","metadata":{"id":"0790ba5e-dca9-4ded-936c-849466df2c95","trusted":true},"outputs":[],"execution_count":null},{"id":"1c2c9183-4536-407a-849b-9f231e27604d","cell_type":"markdown","source":"Wandb используется для стороннего отслеживания процесса обучения. Поскольку удаленные среды вроде google colab могут отрубать среду выполнения при ошибке/завершении обучения с потерей локальных данных (Kaggle, вроде, таким не особо страдает), настоятельно рекомендую подключить аккаунт wandb и раскомментить строки с запуском wandb и сохранением моделей, или же прописать в процесс обучения сохранение данных о модели в другое место, например, на Google Disk.","metadata":{"id":"1c2c9183-4536-407a-849b-9f231e27604d"}},{"id":"5ba163b9-c508-4a28-a351-f90d37a1435a","cell_type":"code","source":"#Здесь предполагается, что у вас в Kaggle в Secrets (во вкладке Add-ons) прописан api-key для Wandb\n#В противном случае, он \"подключается\" к нему (на деле - нет).\n\nimport wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"WANDB\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    anonymous = \"must\"\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c5534cf9-aea7-419f-8f6c-d4b3d6c75c58","cell_type":"code","source":"#Ручной вход в wandb, если у вас есть там аккаунт и есть api-key\n#api_key = 'длинный_ключ_из_профиля_wandb'\n#try:\n#    wandb.login(key=api_key)\n#    anonymous = None\n#except:\n#    anonymous = \"must\"","metadata":{"id":"c5534cf9-aea7-419f-8f6c-d4b3d6c75c58","trusted":true},"outputs":[],"execution_count":null},{"id":"060fe577-728a-4d18-a210-3dd4ad9a9dbb","cell_type":"markdown","source":"Начало обучения. Переименуйте seg_model_name на свое усмотрение. Если используете wandb - раскоментируйте строки внутри функций, отвечающих за обучение (там прописано логирование данных и сохранение моделей) и поменяйте параметры wandb.init.\n\nОбратите внимание, что тяжелые модели (вроде UNet++ с b6 backbone) требуют много видеопамяти на видеокарте, и при запуске локально на обычных не топовых игровых видеокартах CUDA может вылететь с ошибкой \"out of memory\". На Kaggle или Google colab используются видеокарты большим объемом видеопамяти (например, Google Colab и Kaggle используют NVidia Tesla T4 с 16ГБ видеопамяти), поэтому при появлении данной ошибки рекомендую запустить модель в удаленной среде выполнения.\n\nP.S. Хотя на небольших датасетах все обучалось норм, при прогоне на всем датасете с Unet++ b1 backbone у меня loss на train уходил в отрицательные значения (на valid все ок было), так что тут возможна в каком-то месте ошибка. По идее, loss не должен быть отрицательным.","metadata":{"id":"060fe577-728a-4d18-a210-3dd4ad9a9dbb"}},{"id":"a429b2ca-35a6-4b50-99ec-4fed5aea7388","cell_type":"code","source":"for fold in range(1):\n    seg_model_name = 'UNetPPEffb1_LAST'\n    print(f'#'*15)\n    print(f'### Fold: {fold}')\n    print(f'#'*15)\n    run = wandb.init(project='med-segmentation-unetppb1',\n                     config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\n                     anonymous=anonymous,\n                     name=f\"fold-{fold}|dim-{CFG.img_size[0]}x{CFG.img_size[1]}|model-{CFG.model_name}\",\n                     group=CFG.comment,\n                    )\n    train_loader, valid_loader = train_loader,valid_loader\n    model     =   build_model()\n    optimizer = optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n    scheduler =  fetch_scheduler(optimizer)\n\n    model, historys_UNET = run_training(model, optimizer, scheduler,\n                                device=CFG.device,\n                                num_epochs=CFG.epochs)\n    # run.finish()\n    # display(ipd.IFrame(run.url, width=1000, height=720))\n    plt.figure(figsize=(12,9))\n\n    plt.plot(historys_UNET['Train Loss'], label=f'Training Loss')\n    plt.plot(historys_UNET['Valid Loss'], label=f'Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(f'Loss History-model{seg_model_name}')\n\n\n    plt.figure(figsize=(17,9))\n    plt.subplot(1, 2, 1)\n    plt.plot(historys_UNET['Train Dice'], label=f'Training Dice')\n    plt.plot(historys_UNET['Valid Dice'], label=f'Validation Dice')\n    plt.xlabel('Epoch')\n    plt.ylabel('Dice')\n    plt.legend()\n    plt.title(f'Dice History-model{seg_model_name}')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(historys_UNET['Train Jaccard'], label=f'Training Jaccard')\n    plt.plot(historys_UNET['Valid Jaccard'], label=f'Validation Jaccard')\n    plt.xlabel('Epoch')\n    plt.ylabel('Jaccard')\n    plt.legend()\n    plt.title(f'Jaccard History-model{seg_model_name}')\n    plt.show()\n    torch.save(model.state_dict(), f'{seg_model_name}_std.pth')\n    torch.save(model, f'{seg_model_name}.pt')","metadata":{"id":"a429b2ca-35a6-4b50-99ec-4fed5aea7388","outputId":"ae42f8d5-280d-444b-e873-88a881d6e0a2","colab":{"base_uri":"https://localhost:8080/","height":263},"trusted":true},"outputs":[],"execution_count":null},{"id":"3a41011c-7475-48dc-8dc1-a2ecab7aa5a3","cell_type":"code","source":"history = historys_UNET\nprint(f\"\"\" MODEL: {seg_model_name} Train DICE MAX:{max(history['Train Dice'])} IOU MAX:{max(history['Train Jaccard'])} Train Loss MIN:{min(history['Train Loss'])}\n        MODEL: {seg_model_name} VAL DICE MAX:{max(history['Valid Dice'])} VAL IOU MAX:{max(history['Valid Jaccard'])} VAL Loss MIN:{min(history['Valid Loss'])} \\n \"\"\")","metadata":{"id":"3a41011c-7475-48dc-8dc1-a2ecab7aa5a3","trusted":true},"outputs":[],"execution_count":null},{"id":"d0d4cfb2-a5d1-4712-b893-75e116fef7a3","cell_type":"code","source":"!zip model_weights.zip *.pth *.bin  # Архивируйте все веса","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8f973aed-09e2-4bb6-be78-aa6cf441cf77","cell_type":"code","source":"from IPython.display import FileLink\nFileLink('model_weights.zip')  # Ссылка для скачивания","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dce7e42e-7530-4293-8746-c5526650ca99","cell_type":"markdown","source":"Очистите кэш GPU перед обучением новой модели","metadata":{"id":"dce7e42e-7530-4293-8746-c5526650ca99"}},{"id":"be9cf7cc-0616-4984-ad7b-e0f1f69cf00c","cell_type":"code","source":"gpus = GPUtil.getGPUs()\nGPUtil.showUtilization()\nfor i in range(len(gpus)):\n    gpu = gpus[i]\n    free_memory = gpu.memoryFree\ntorch.cuda.empty_cache()","metadata":{"id":"be9cf7cc-0616-4984-ad7b-e0f1f69cf00c","trusted":true},"outputs":[],"execution_count":null},{"id":"6af943e2-2ba4-42f7-a6e3-a5f1959fa030","cell_type":"markdown","source":"# Валидация","metadata":{"id":"6af943e2-2ba4-42f7-a6e3-a5f1959fa030"}},{"id":"2476c29d-3a5b-4cf7-86c9-a730e9cedb2e","cell_type":"markdown","source":"Очистка кэша GPU","metadata":{"id":"2476c29d-3a5b-4cf7-86c9-a730e9cedb2e"}},{"id":"b1b51695-faff-4c46-b794-3765c7527826","cell_type":"code","source":"gpus = GPUtil.getGPUs()\nGPUtil.showUtilization()\nfor i in range(len(gpus)):\n    gpu = gpus[i]\n    free_memory = gpu.memoryFree\ntorch.cuda.empty_cache()","metadata":{"id":"b1b51695-faff-4c46-b794-3765c7527826","trusted":true},"outputs":[],"execution_count":null},{"id":"dca5fcf9-cca2-4028-ab8f-47cd6b4b28d0","cell_type":"code","source":"valid_dataset = BuildDataset(x_val, y_val, transforms=data_transforms['valid'])\nvalid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, pin_memory=True)","metadata":{"id":"dca5fcf9-cca2-4028-ab8f-47cd6b4b28d0","trusted":true},"outputs":[],"execution_count":null},{"id":"1db24af1-6d13-44d7-9b7b-b1fb564f9664","cell_type":"markdown","source":"Загружаем веса модели. В weight_c1 укажите название файла с весами необходимой модели. В параметрах model укажите тот encoder_name, с которым обучались веса модели.","metadata":{"id":"1db24af1-6d13-44d7-9b7b-b1fb564f9664"}},{"id":"ca9e0a4b-17d7-4507-ba1f-9eaace145ad7","cell_type":"code","source":"weight_c1 = 'UNetPPEffb1_LAST_std.pth'\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"efficientnet-b1\", encoder_weights=None, in_channels=1, classes=1)\nmodel.to('cuda')\nmodel.load_state_dict(torch.load(\n    weight_c1, map_location='cuda'))\nmodel.eval()","metadata":{"id":"ca9e0a4b-17d7-4507-ba1f-9eaace145ad7","trusted":true},"outputs":[],"execution_count":null},{"id":"b157beb0-f617-435e-ad14-80cbe6aa6b18","cell_type":"code","source":"device='cpu'","metadata":{"id":"b157beb0-f617-435e-ad14-80cbe6aa6b18","trusted":true},"outputs":[],"execution_count":null},{"id":"6e230de6-0d8f-453b-bba2-c94a140ff82f","cell_type":"code","source":"preds = []\nfor step, (images, masks) in enumerate(valid_loader):\n        images  = images.to(device, dtype=torch.float).cpu()\n        masks   = masks.to(device, dtype=torch.float).cpu()\n        models=model.cpu()\n\n        for fold in range(1):\n            # model = load_model(f\"best_epoch-{fold:02d}.bin\")\n            with torch.no_grad():\n                pred = models(images).cpu()\n                pred = (nn.Sigmoid()(pred)>0.5).double()\n\n                #preds = torch.argmax(nn.Sigmoid()(pred),axis=1).double()\n                #val_dice = dice_coef(mask, pred).cpu().detach().numpy()\n                #val_jaccard = iou_coef(mask, pred).cpu().detach().numpy()\n                #val_scores.append([val_dice, val_jaccard])\n\n\n                print( sum(sum(sum(sum(pred)))))\n            preds.append(pred)\n\n        images  = images.cpu().detach()\n        preds = torch.mean(torch.stack(preds, dim=0), dim=0).cpu().detach()\n        break","metadata":{"id":"6e230de6-0d8f-453b-bba2-c94a140ff82f","trusted":true},"outputs":[],"execution_count":null},{"id":"565a9e29-5c63-4ebd-a47b-57b516b34121","cell_type":"code","source":"#Альтернативный расчет Dice\n\ndef dice_coef_2(y_true, y_pred):\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n    smooth = 0.0001\n    intersection = (y_true*y_pred).sum()\n    print(intersection)\n    print(((y_true_f).sum() + (y_pred_f).sum() + smooth))\n    print((y_true_f).sum())\n    print((y_pred_f).sum())\n    return (2. * intersection + smooth) / ((y_true_f).sum() + (y_pred_f).sum() + smooth)","metadata":{"id":"565a9e29-5c63-4ebd-a47b-57b516b34121","trusted":true},"outputs":[],"execution_count":null},{"id":"3c544d15-37d7-488a-bc96-9aca707a11f8","cell_type":"code","source":"#Метрика Dice-Sørensen coefficient\n\ndice_coef(masks, preds).cpu().detach().numpy()","metadata":{"id":"3c544d15-37d7-488a-bc96-9aca707a11f8","trusted":true},"outputs":[],"execution_count":null},{"id":"770da9aa-f5d6-49b2-9486-3eb20c5d3c42","cell_type":"code","source":"#Метрика IoU (Jaccard index)\n\niou_coef(masks, preds).cpu().detach().numpy()","metadata":{"id":"770da9aa-f5d6-49b2-9486-3eb20c5d3c42","trusted":true},"outputs":[],"execution_count":null},{"id":"c2896134-55bf-4c10-90f7-599391aea051","cell_type":"markdown","source":"Результаты работы модели: изображение, изображение + размеченая маска сегментации, изображение + предсказанная маска сегментации.\n\nЗдесь можно визуально оценить, насколько хорошо (или плохо) ваша модель работает.","metadata":{"id":"c2896134-55bf-4c10-90f7-599391aea051"}},{"id":"f2006166-eec3-414b-920c-56ef992dd23b","cell_type":"code","source":"plot_batch(images, images, size=5)\nplot_batch(images, masks, size=5)\nplot_batch(images, preds, size=5)","metadata":{"id":"f2006166-eec3-414b-920c-56ef992dd23b","trusted":true},"outputs":[],"execution_count":null},{"id":"59ea9b55-d83c-4c52-9eda-74e4d76d66c2","cell_type":"code","source":"","metadata":{"id":"59ea9b55-d83c-4c52-9eda-74e4d76d66c2","trusted":true},"outputs":[],"execution_count":null},{"id":"af6f1e0a-6c27-4639-9c7b-398bac192107","cell_type":"code","source":"","metadata":{"id":"af6f1e0a-6c27-4639-9c7b-398bac192107","trusted":true},"outputs":[],"execution_count":null}]}